{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2: Black and White Colorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Loading and modifying dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor') #Sets default tensor value to float\n",
    " \n",
    "def loadImgsToList():\n",
    "    img_dir = \"./AllImages/face_images/*.jpg\" #Function which loads in the face images and returns a list containing RGB values, as well as the number of images read\n",
    "    files = sorted(glob.glob(img_dir))\n",
    "\n",
    "    data = []\n",
    "    numImages = 0\n",
    "    for f1 in files:\n",
    "        image = cv2.imread(f1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #cv2 stores colors as BGR instead of RGB for some reason     \n",
    "        data.append(image)\n",
    "        numImages += 1 #probably unnecessary but I like to avoid using len() for arrays/lists of multiple dimensions\n",
    "    \n",
    "    return data, numImages #Capture return as array, number = loadImgsToList()\n",
    "   \n",
    "def createTensor(data, numImages): #Function to create an Tensor from a data list, data is unshuffled and unedited from raw\n",
    "    shape = (numImages, 128, 128, 3) #Shape of the tensor = (number of images, height, width, number of channels), it was recommended online to do it in this order\n",
    "    tempTensor = torch.zeros(shape) #Populate empty tensor of correct size\n",
    "\n",
    "    count = 0\n",
    "    for i in data:\n",
    "        curData = torch.from_numpy(data[count]) #converts \n",
    "        tempTensor[count] = curData\n",
    "        count += 1\n",
    "    \n",
    "    return tempTensor\n",
    "\n",
    "def shuffleTensor(tempTensor, numImages):\n",
    "    perm = torch.randperm(numImages) #Gives an array which is a permutation of the numbers [0:numImages] EG: 0, 1, 2, 3 can become 0, 3, 1, 2\n",
    "    shape = (numImages, 128, 128, 3) \n",
    "    \n",
    "    rgbTensor = torch.zeros(shape) #holder\n",
    "    for i in perm: #set holder value to its permutated counterpart\n",
    "        rgbTensor[i] = tempTensor[perm[i]]\n",
    "    \n",
    "    return rgbTensor\n",
    "\n",
    "def randomCrop(inputTensor):\n",
    "    length = np.random.randint(70,115) #crop size, minimum image size after crop is 70x70, max is 115x115, arbitrary and can/should be adjusted\n",
    "    randCrop = T.RandomCrop(size = (length, length))\n",
    "    resized = T.Resize(size = (128, 128)) #resizes images to 128x128\n",
    "    tempTensor = inputTensor.permute(2, 0, 1) #because randomcrop doesn't like the 3 at the end\n",
    "    tempTensor = resized(randCrop(tempTensor))\n",
    "    tempTensor = tempTensor.permute(1, 2, 0) #fixing the other permute\n",
    "    return tempTensor\n",
    "\n",
    "def horizontalFlip(inputTensor):\n",
    "    return torch.fliplr(inputTensor) #convenient built in\n",
    "\n",
    "def scaleRGBValues(inputTensor):\n",
    "    scalar = np.random.uniform(.6, 1)\n",
    "    return inputTensor * scalar #This was way too complicated before\n",
    "\n",
    "def populateAndScaleDataset(data, numImages, scaleFactor, saveTensor):\n",
    "    myTensor = createTensor(data, numImages) #Create and shuffle a tensor from the loaded in data\n",
    "    #myTensor = shuffleTensor(myTensor, numImages)\n",
    "    \n",
    "    newShape = (scaleFactor*numImages, 128, 128, 3) #Create a holder for the new, scaled Tensor\n",
    "    newTensor = torch.zeros(newShape)\n",
    "    \n",
    "    count = 0 #horizontal counter\n",
    "    for i in range(numImages): #for each data image\n",
    "        for j in range(scaleFactor): #make {scaleFactor} more\n",
    "            if j == 0: #Include the original\n",
    "                newTensor[count] = myTensor[i]\n",
    "            else: #else, randomly change the rest\n",
    "                tempTensor = myTensor[i] #initialize to original\n",
    "                \n",
    "                val1 = np.random.uniform(0, 1) #rand val between 0 and 1\n",
    "                if val1 >= .5: #half of the time, flip it\n",
    "                    tempTensor = horizontalFlip(tempTensor)\n",
    "                \n",
    "                val2 = np.random.uniform(0, 1)\n",
    "                if val2 >= .5: #half of the time, crop it\n",
    "                    tempTensor = randomCrop(tempTensor)\n",
    "                    \n",
    "                val3 = np.random.uniform(0, 1)\n",
    "                if val3 >= .5: #half of the time, scale it\n",
    "                    tempTensor = scaleRGBValues(tempTensor)\n",
    "            \n",
    "                newTensor[count] = tempTensor #add it to the holder\n",
    "            count += 1    #increment count\n",
    "            \n",
    "            #NOTE: obviously, the values for when to alter the new images can be adjusted. It's done this way so that multiple transformations can be done on one image\n",
    "            # Has the flaw of sometimes repeating images, which may be bad for the dataset. Can be easily fixed by adding a default case at the end, but I have omitted that for now\n",
    "            \n",
    "    if saveTensor:\n",
    "        x = \"000000\" \n",
    "        for i in range(len(newTensor)):\n",
    "            filename = r\"./AllImages/augmented/rgb_image\" + x + \".jpg\"\n",
    "            x =  str(int(x) + 1).zfill(len(x)) #maintains 6 digits at end\n",
    "\n",
    "            img = newTensor[i].numpy()\n",
    "            cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #convert to BGR so cv2 can save it\n",
    "            s = cv2.imwrite(filename, img)\n",
    "\n",
    "            if not s:\n",
    "                print(s) #print failure\n",
    "\n",
    "    return newTensor\n",
    "\n",
    "def convertToLab(mytensor, saveTensor):\n",
    "    holderTensor = torch.zeros(mytensor.shape) #holder tensor to return\n",
    "    \n",
    "    x = \"000000\" #used in filenames\n",
    "    for i in range(len(mytensor)):\n",
    "        test = cv2.cvtColor(mytensor[i].numpy()/255.0, cv2.COLOR_RGB2Lab) #actual conversion\n",
    "        holderTensor[i] = torch.from_numpy(test) #save to holder\n",
    "        \n",
    "        if saveTensor:\n",
    "            filenameL = r\"./AllImages/L/L_image\" + x + \".jpg\" #initialize filenames\n",
    "            filenameA = r\"./AllImages/a/a_image\" + x + \".jpg\"\n",
    "            filenameB = r\"./AllImages/b/b_image\" + x + \".jpg\"\n",
    "            x =  str(int(x) + 1).zfill(len(x)) #maintains 6 digits at end and increments x\n",
    "\n",
    "            L, a, b = cv2.split(test) #split L, a, and b\n",
    "            L_s = cv2.imwrite(filenameL, L) #and save them\n",
    "            a_s = cv2.imwrite(filenameA, a)\n",
    "            b_s = cv2.imwrite(filenameB, b)\n",
    "\n",
    "            if not L_s or not a_s or not b_s :\n",
    "                print(L_s, a_s, b_s) #print failure\n",
    "            \n",
    "    return holderTensor\n",
    "\n",
    "def displayImage(data):\n",
    "    image = data\n",
    "    #Show the image with matplotlib\n",
    "    plt.imshow(image/255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call data initialization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize dataset, scale it\n",
    "data, numImgs = loadImgsToList()\n",
    "sf = 10\n",
    "\n",
    "tensor = populateAndScaleDataset(data, numImgs, sf, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensor = convertToLab(tensor, False) #convert color space (prev 2 functions also save files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shuffleTensor(tensor, numImgs*sf) #Shuffle after saving to preserve order in files (cataloging purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def splitChannelsAsTensors(dataset):\n",
    "    L = torch.zeros((len(dataset), 128, 128))\n",
    "    a = torch.zeros((len(dataset), 128, 128))\n",
    "    b = torch.zeros((len(dataset), 128, 128))\n",
    "    \n",
    "    dataset = dataset.permute(0, 3, 1, 2)\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        L[i] = dataset[i][0]/100\n",
    "        a[i] = dataset[i][1]\n",
    "        b[i] = dataset[i][2]\n",
    " \n",
    "    L = L.reshape(len(dataset), 128, 128, 1)\n",
    "    a = a.reshape(len(dataset), 128, 128, 1)\n",
    "    b = b.reshape(len(dataset), 128, 128, 1)\n",
    "\n",
    "    return L, a, b\n",
    "\n",
    "net = nn.Sequential(\n",
    "        nn.Conv2d(1, 64, 5, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      ")\n",
      "torch.Size([7500, 128, 128, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 5, 5], expected input[7500, 128, 128, 1] to have 1 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/21316448/ipykernel_112843/3560455818.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 5, 5], expected input[7500, 128, 128, 1] to have 1 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "print(net)\n",
    "\n",
    "dataL, dataA, dataB = splitChannelsAsTensors(dataset)\n",
    "\n",
    "print(dataL.shape)\n",
    "\n",
    "\n",
    "print(net(dataL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
